{
 "cells": [
  {
   "cell_type": "raw",
   "id": "67072fa4-5fb1-413c-afa1-862d9cc8ea8d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Ce script est pour l'extraction de 1000 phrases pour crée un dataset  \n",
    "de l'experimentation + la génération de bruits sur ces phrases\n",
    "selon trois niveau : faible , moyen et fort puis l'enregistrer sur un fichiers json et csv qui contient 5 colonnes : phrase original, tokenisation de référence , bruit faible , bruit moyen , bruit fort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cce9fcc-4be8-4f97-b67c-5a42631a93a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON généré : dataset_bruite.json\n",
      "✅ CSV généré : dataset_bruite.csv avec 1000 phrases.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# FONCTIONS DE BRUIT\n",
    "\n",
    "def bruit_ocr(text):\n",
    "    # Remplace certains caractères le bruit d'OCR\n",
    "    return text.replace(\"l\", \"1\").replace(\"O\", \"0\").replace(\"o\", \"0\").replace(\"I\", \"1\").replace(\"œ\", \"oe\")\n",
    "\n",
    "def bruit_mots_colles(text):\n",
    "    mots = text.split()\n",
    "    if len(mots) > 2:\n",
    "        i = random.randint(0, len(mots)-2)\n",
    "        mots[i] = mots[i] + mots[i+1]\n",
    "        del mots[i+1]\n",
    "    return \" \".join(mots)\n",
    "\n",
    "def bruit_fraction(text):\n",
    "    mots = text.split()\n",
    "    i = random.randint(0, len(mots)-1)\n",
    "    mot = mots[i]\n",
    "    if len(mot) > 4:\n",
    "        cut = len(mot)//2\n",
    "        mots[i] = mot[:cut] + \" \" + mot[cut:]\n",
    "    return \" \".join(mots)\n",
    "\n",
    "def bruit_apostrophe(text):\n",
    "    # Modifie l'apostrophe\n",
    "    return text.replace(\"'\", \"’\").replace(\"’\", \"ʼ\")\n",
    "\n",
    "def bruit_tiret(text):\n",
    "    return text.replace(\"-\", \"—\")\n",
    "\n",
    "def bruit_points(text):\n",
    "    return text.replace(\"...\", \"…\")\n",
    "\n",
    "def bruit_guillemets(text):\n",
    "    return text.replace('\"', '«').replace(\"'\", \"»\")\n",
    "\n",
    "def bruit_note(text):\n",
    "    mots = text.split()\n",
    "    i = random.randint(0, len(mots)-1)\n",
    "    mots[i] = mots[i] + str(random.randint(1,9))\n",
    "    return \" \".join(mots)\n",
    "\n",
    "BRUITS = [bruit_ocr, bruit_mots_colles, bruit_fraction, bruit_apostrophe,\n",
    "          bruit_tiret, bruit_points, bruit_guillemets, bruit_note]\n",
    "\n",
    "def appliquer_bruit(text, niveau=\"faible\"):\n",
    "    if niveau == \"faible\":\n",
    "        f = random.choice(BRUITS)\n",
    "        return f(text)\n",
    "    elif niveau == \"moyen\":\n",
    "        fs = random.sample(BRUITS, 2)\n",
    "        for f in fs:\n",
    "            text = f(text)\n",
    "        return text\n",
    "    elif niveau == \"fort\":\n",
    "        fs = random.sample(BRUITS, 4)\n",
    "        for f in fs:\n",
    "            text = f(text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# EXTRACTION DES PHRASES\n",
    "\n",
    "def extraire_phrases_conllu(fichier, n=1000):\n",
    "    phrases = []\n",
    "    current_tokens = []\n",
    "    current_text = None\n",
    "\n",
    "    with open(fichier, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# text =\"):\n",
    "                # Sauvegarde la phrase précédente\n",
    "                if current_text and current_tokens:\n",
    "                    phrases.append((current_text, current_tokens))\n",
    "                # Nouvelle phrase\n",
    "                current_text = line.split(\"=\", 1)[1].strip()\n",
    "                current_tokens = []\n",
    "            elif line and not line.startswith(\"#\"):\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) > 1 and parts[0].isdigit() and \"-\" not in parts[0] and \".\" not in parts[0]:\n",
    "                    current_tokens.append(parts[1])\n",
    "        # Ajouter la dernière phrase\n",
    "        if current_text and current_tokens:\n",
    "            phrases.append((current_text, current_tokens))\n",
    "\n",
    "    # Sélection : début, milieu, fin\n",
    "    total = len(phrases)\n",
    "    selection = phrases[:n//3] + phrases[total//2:total//2 + n//3] + phrases[-n//3:]\n",
    "    return selection[:n]\n",
    "\n",
    "# dataset complet\n",
    "\n",
    "def generer_datasets(fichier_conllu, sortie_json=\"dataset_bruite.json\", sortie_csv=\"dataset_bruite.csv\", n=1000):\n",
    "    phrases = extraire_phrases_conllu(fichier_conllu, n=n)\n",
    "    dataset = []\n",
    "\n",
    "    for phrase, tokens in phrases:\n",
    "        entree = {\n",
    "            \"original\": phrase,\n",
    "            \"reference_tokens\": tokens,\n",
    "            \"bruit_faible\": appliquer_bruit(phrase, \"faible\"),\n",
    "            \"bruit_moyen\": appliquer_bruit(phrase, \"moyen\"),\n",
    "            \"bruit_fort\": appliquer_bruit(phrase, \"fort\"),\n",
    "        }\n",
    "        dataset.append(entree)\n",
    "\n",
    "    # Sauvegarde JSON\n",
    "    with open(sortie_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Sauvegarde CSV\n",
    "    with open(sortie_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"original\", \"reference_tokens\", \"bruit_faible\", \"bruit_moyen\", \"bruit_fort\"])\n",
    "        for entree in dataset:\n",
    "            writer.writerow([\n",
    "                entree[\"original\"],\n",
    "                \" \".join(entree[\"reference_tokens\"]),\n",
    "                entree[\"bruit_faible\"],\n",
    "                entree[\"bruit_moyen\"],\n",
    "                entree[\"bruit_fort\"]\n",
    "            ])\n",
    "\n",
    "    print(f\"✅ JSON généré : {sortie_json}\")\n",
    "    print(f\"✅ CSV généré : {sortie_csv} avec {len(dataset)} phrases.\")\n",
    "\n",
    "# LANCEMENT\n",
    "\n",
    "generer_datasets(\"fr_gsd-ud-train.conllu\", n=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6bb20-182d-4b87-b323-e3b9cb3026d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
