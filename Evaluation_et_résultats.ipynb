{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaSgqnV4KiPG"
      },
      "source": [
        "Ce script permet d’évaluer les performances des tokeniseurs (SpaCy, Stanza et BERT) sur le corpus bruité.\n",
        "Il compare les séquences de tokens générées par chaque tokeniseur avec la séquence de référence, en utilisant la métrique Exact Match (EM), après il calcule la précison , rappel et F mesure .\n",
        "À noter : dans notre configuration, BERT ne segmente pas les phrases en mots mais conserve chaque entrée comme un seul token. Cela a un impact sur l’évaluation, puisque ses résultats ne sont pas directement comparables à ceux de SpaCy et Stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ18oOJ1JQKv",
        "outputId": "29294239-7968-4b8e-e71b-e11ccf8863a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Résultats détaillés : dataset_bruit_tokenizes.csv\n",
            "✅ Résumé global : resume_tokenizers.csv\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_tokenization(reference_tokens, predicted_tokens):\n",
        "    \"\"\"Compare deux listes de tokens et calcule Exact Match, Précision, Rappel, F1\"\"\"\n",
        "    exact_match = 1 if reference_tokens == predicted_tokens else 0\n",
        "\n",
        "    # Convert lists to sets for efficient comparison\n",
        "    ref_set = set(reference_tokens)\n",
        "    pred_set = set(predicted_tokens)\n",
        "\n",
        "    true_positives = len(ref_set.intersection(pred_set))\n",
        "    precision = true_positives / len(pred_set) if pred_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return exact_match, precision, recall, f1\n",
        "\n",
        "\n",
        "# Charger ton fichier JSON\n",
        "with open(\"dataset_bruit_tokenized.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Fichier CSV détaillé\n",
        "with open(\"dataset_bruit_tokenizes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Phrase\", \"Bruit\", \"Tokenizer\", \"ExactMatch\", \"Précision\", \"Rappel\", \"F1\"])\n",
        "\n",
        "    # Pour les moyennes\n",
        "    scores = defaultdict(list)\n",
        "\n",
        "    # Define the noise levels to iterate over\n",
        "    noise_levels = [\"original\", \"bruit_faible\", \"bruit_moyen\", \"bruit_fort\"]\n",
        "    tokenizers = [\"spacy\", \"stanza\", \"bert\"]\n",
        "\n",
        "    for item in data:\n",
        "        reference = item[\"reference_tokens\"]\n",
        "\n",
        "        for bruit_level in noise_levels:\n",
        "            phrase = item[bruit_level] \n",
        "\n",
        "            for tokenizer_name in tokenizers:\n",
        "                # Construct the key for the predicted tokens\n",
        "                predicted_key = tokenizer_name\n",
        "                if bruit_level != \"original\":\n",
        "                    predicted_key = f\"{tokenizer_name}\" # Predicted tokens are grouped by tokenizer name in the JSON\n",
        "\n",
        "                predicted = item[predicted_key][bruit_level] # Access the tokens for the specific noise level\n",
        "\n",
        "                exact, prec, rec, f1 = evaluate_tokenization(reference, predicted)\n",
        "\n",
        "                writer.writerow([phrase, bruit_level, tokenizer_name, exact, prec, rec, f1])\n",
        "\n",
        "                # Sauvegarde pour moyennes\n",
        "                scores[(bruit_level, tokenizer_name)].append((exact, prec, rec, f1))\n",
        "\n",
        "\n",
        "# Calcul des moyennes\n",
        "with open(\"resume_tokenizers.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Bruit\", \"Tokenizer\", \"ExactMatch_moy\", \"Précision_moy\", \"Rappel_moy\", \"F1_moy\"])\n",
        "\n",
        "    for (bruit, tokenizer), vals in scores.items():\n",
        "        if vals:\n",
        "            exacts, precs, recs, f1s = zip(*vals)\n",
        "            writer.writerow([\n",
        "                bruit, tokenizer,\n",
        "                sum(exacts)/len(exacts),\n",
        "                sum(precs)/len(precs),\n",
        "                sum(recs)/len(recs),\n",
        "                sum(f1s)/len(f1s)\n",
        "            ])\n",
        "\n",
        "print(\" Résultats détaillés : dataset_bruit_tokenizes.csv\")\n",
        "print(\" Résumé global : resume_tokenizers.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
